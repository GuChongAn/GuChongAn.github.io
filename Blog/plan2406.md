---
name: [总结] 2024年6月学习总结
date: 20240701
description: 对2024年6月学习情况的总结
keywords:plan
---

来做个6月份学习的总结以及做7月份的规划，6月的总结分成两个部分，

- 论文阅读，看的论文的总结和评价
- 学习总结，专业课程或者其他方面的学习总结

### 一 论文阅读

6月看来**20**篇论文，首先是**6**篇多模态QA的文章，

- [MultiModalQA: Complex Question Answering over Text, Tables and Images ](https://arxiv.org/abs/2104.06039)（ICLR21）
- [WebQA: Multihop and Multimodal QA ](https://arxiv.org/abs/2109.00590)（CVPR22）
- [MMCoQA: Conversational Question Answering over Text, Tables, and Images ](https://aclanthology.org/2022.acl-long.290/)（ACL22）
- [MMToM-QA: Multimodal Theory of Mind Question Answering ](https://arxiv.org/abs/2401.08743)（ACL24）
- [CABINET: Content Relevance-based Noise Reduction for Table Question Answering ](https://openreview.net/pdf?id=SQrHpTllXa)（ICLR24）
- [EQA-MX: Embodied Question Answering using Multimodal Expression ](https://openreview.net/pdf?id=7gUrYE50Rb)（ICLR24）

总结如下，

| 模型         | 发布年份 | 多模态数据                           | 上下文             |
| ------------ | -------- | ------------------------------------ | ------------------ |
| MultiModalQA | ICLR 21  | 不融合，迭代处理，每步处理一个模态   | 数据集提供         |
| WebQA        | CVPR 22  | 图片文本编码后一起塞给LLM            | 检索（问LLM）      |
| MMCoQA       | ACL 22   | 问题实际都是单模态                   | 检索（计算相似度） |
| MMToM-QA     | ACL 24   | 用LLM转为符号表达，回答问题用BIP-ALM | 数据集提供         |
| CABINET      | ICLR 24  | 只有表格数据，计算子表相关分数       | 数据集提供         |
| EQA-MX       | ICLR 24  | 通过VQ的方法把图像离散化             | 数据集提供         |

然后是补基础论文，看了**8**篇，

- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
- [Masked Autoencoders Are Scalable Vision Learners ](https://arxiv.org/abs/2111.06377)（MAE）
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ](https://arxiv.org/abs/2010.11929)（ViT）

- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Training language models to follow instructions with human feedback ](https://arxiv.org/abs/2203.02155)（InstructGPT）
- [Learning Transferable Visual Models From Natural Language Supervision ](https://arxiv.org/abs/2103.00020)（CLIP，ICML21）
- [CoCa: Contrastive Captioners are Image-Text Foundation Models ](https://paperswithcode.com/paper/coca-contrastive-captioners-are-image-text)（arxiv22 引用1013）

然后看了CVPR和ICLR24的best paper中的**4**篇

- [Rich Human Feedback for Text-to-Image Generation ](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.pdf)（CVPR 24 best paper）
- [BioCLIP: A Vision Foundation Model for the Tree of Life ](https://openaccess.thecvf.com/content/CVPR2024/papers/Stevens_BioCLIP_A_Vision_Foundation_Model_for_the_Tree_of_Life_CVPR_2024_paper.pdf)（CVPR 24 best student paper）
- [Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods ](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Comparing_the_Decision-Making_Mechanisms_by_Transformers_and_CNNs_via_Explanation_CVPR_2024_paper.pdf)（CVPR 24 honorable mentions）
- [Vision Transformers Need Registers ](https://openreview.net/pdf?id=2dnO3LLiJ1)（ICLR 24 outstanding paper）

还看了**2**篇文本总结的文章，没啥用，

- [ChatGPT as a Factual Inconsistency Evaluator for Text Summarization](https://arxiv.org/abs/2303.15621)
- [Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization](https://arxiv.org/abs/2302.08081)

### 二 学习总结

本来计划看看CSAPP和数学分析的，结果懈怠了，没看，下个月一定捡起来

### 三 7月规划

首先是论文阅读的计划，还是有点不特清楚看啥，

- 先把找到的那片多模态的文章看了
- 然后还是找会议的best paper看

然后是学习上的规划（虽然6月也写的这两个），

- 《CSAPP》至少看完第四章
- 数学分析，计划把《数学分析新讲》第一本书看完

