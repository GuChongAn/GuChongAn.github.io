---
name: [Read Paper] 9.2-9.18 论文阅读（RAG/KICGPT/StructGPT/Seq2SeqKGC/KGLLMTest）
date: 20240918
description: 
keywords: readpaper
---

看了RAG的原始论文，

- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)（NeurIPS 20）

看了几篇知识图谱补全（Knowledge Graph Completion）和知识图谱QA的论文，

- [KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion](https://aclanthology.org/2023.findings-emnlp.580/)（EMNLP findings 23）
- [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://aclanthology.org/2023.emnlp-main.574/)（EMNLP 23）
- [Sequence-to-Sequence Knowledge Graph Completion and Question Answering](https://aclanthology.org/2022.acl-long.201.pdf)（ACL 22）
- [Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family](https://arxiv.org/abs/2303.07992)（ISWC 23）

## [NeurIPS 20] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

#### 1）问题

![image-20240909221249088](../image/2024/image-20240909221249088.png)

#### 2）方法

![image-20240909221321258](../image/2024/image-20240909221321258.png)

本文提出了两种RAG的范式，

- RAG-Sequence，就是现在常见的形式，检索出文档，输入LLM输出答案
- RAG-Token，对于每个输出Token的生成检索不同的文档辅助

这里了解到一个之前不知道的知识，文本生成解码策略，对于现在的文本生成模型，输入文本序列x1,x2,x3...模型会逐步生成xm,xm+1...，生成概率为，

<img src="../image/2024/image-20240909222118863.png" alt="image-20240909222118863" style="zoom:50%;" />

我们最终目的是让生成正确文本的概率最大， 所以就有了如下的解码策略，

- 贪心搜索，顾名思义

- beam search（集束搜索），每步保留k个概率最大的文本序列，本文使用的就是这个方法
- 等等

## [EMNLP findings 23] KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion

#### 1）问题

之前知识图谱补全的工作可以分为，

- Triple-based approaches

我理解基于三元组的方法（Triple-based）就是对于一个待补全的三元组，你的回答只能基于训练的时候已知的知识图谱信息，所以会存在**对于长尾实体**（Long-tail entities，一个知识图谱中某些实体出现次数很少，但是这类实体很多）**性能不好**的问题

- Text-based approaches

利用外部的文本信息来补全三元组（例如使用LLM预训练的知识），问题在于**需要特定领域的知识或者对LLM的微调**，对于Text-based中的LLM相关的方法，具体还有**1）LLM回答不受限制；2）LLM输入长度首先；3）当时不存在高效的prompt**等问题（我觉得这几个问题就有点牵强了）

#### 2）方法

本文提出了一套KGC的框架，具体如下图，

![image-20240909220017615](../image/2024/image-20240909220017615.png)

可以分为以下几步

- 输入(h,r,?)给KGC检索器，得到最初的相关实体

- 让大模型给上一步的结果中最相关的m个实体排序，这里有两个细节
  - 使用prompt template将处理原始的输入
  - 从anaplog pool和supplemet pool中选择示例，前者是r和输入相同的三元组，后者是h实体相同的

- Text Self-Alignment，知识图谱中的关系通常表示为xx_xxx_xxx类似的公式，这里将其转为自然语言

#### 3）实验

- 数据集：FB15k-237和WN18RR
- 实现细节：KGC Retriever使用RotatE，LLM使用gpt-3.5-turbo
- 评价指标：Hit@1,3,10和MRR（Mean Reciprocal Rank）

其中后者，平均倒数秩，是指第一个正确答案的排名的倒数，公式如下，

<img src="../image/2024/image-20240909220904409.png" alt="image-20240909220904409" style="zoom:50%;" />

- 主体实验如下图，

![image-20240909221007235](../image/2024/image-20240909221007235.png)

- 消融实验如下图，

![image-20240909221025979](../image/2024/image-20240909221025979.png)

## [EMNLP 23] StructGPT: A General Framework for Large Language Model to Reason over Structured Data

#### 1）问题

- LLM回答问题可能会有**幻觉**或者需要**特定领域的知识**
- 所以，用外部数据来增强LLM，其中结构化数据经常被使用，但是LLM不能很好的理解结构数据，之前的方法把结构数据直接转成文本输入给大模型，这种方法受限于**大模型的输入长度**
- （动机）现在有Tool manipulation strategy 来增强LLM的能力，可以设计Tool来处理外部结构数据，完成Tool的设计需要解决两个问题
  - **对特定任务设计合适的接口**
  - **LLM如何使用结构数据推理**

#### 2）方法

根据前述的两个问题，本文提出了Iterative Reading-then-Reasoning（IRR）的框架，如下图

![image-20240910212435693](../image/2024/image-20240910212435693.png)

主要分为Reading和Reasoning两个步骤，

- Reading

对Knowledge Graph，Table和Database三种结构数据分别构建了处理接口，针对其特点提取有用的信息

- Reasoning

Read到的结构数据会经过LLM提取有效部分，然后根据问题种类得到最后的答案

#### 3）实验

- 数据集（KGQA）：WebQuestionsSP（WebQSP）和MetaQA
- 评价指标（KGQA）：Hits@1
- LLM：Davinici-003和gpt-3.5-turbo

KGQA相关的主要结果如下，

![image-20240910212904846](../image/2024/image-20240910212904846.png)

这里我发现了个问题，好像我之前对KGQA的理解有误，这里的两个数据集都是有一个问题，这个问题中有个中心实体，模型的输出应该是在知识图谱中找到另一个实体，输出实体在问题实体的几跳范围之内（我记得KQA pro里面讲自己的第二个有点是问题的多样性，现在大概理解到这意思了）

然后本文提供了实例，如下图，

![image-20240910213220705](../image/2024/image-20240910213220705.png)

## [ACL 22] Sequence-to-Sequence Knowledge Graph Completion and Question Answering

#### 1）问题

之前都用Knowledge Graph Embeddings（KGEs）来解决Knowledge Graph Completion问题，但是KGEs的方法在拥有大量实体的知识图谱上会导致模型特别大

#### 2）方法

说起来很简单，本文实际上是提出了一种大模型训练的目标函数，通过训练把知识图谱的信息训到大模型中去，具体来说，有如下步骤，

- Text mapping and Verbalization，把需要补全的三元组转成自然语言
- 使用上一步得到文本作为模型的输入，让模型输出可能补全的实体

本文模型和KGEs的对比如下图，

![image-20240910214149035](../image/2024/image-20240910214149035.png)

#### 3）实验

- 数据集（KGQA方面）：MetaQA，WebQSP和ComplexWebQuestioon（CWQ），前两者之前都见过，最后的CWQ应该有点类似KQA pro，有些复杂问题

实验的主体结果如下表，

![image-20240910214414680](../image/2024/image-20240910214414680.png)

![image-20240910214420307](../image/2024/image-20240910214420307.png)

## [ISWC 23] Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family

#### 1）问题

本文实际是一篇实验报告，比较了LLM在不依赖对应数据集KG的情况下和传统方法在KBQA任务上的性能，解决的问题自然是之前没有这类型的综合测试

#### 2）方法

本文构建了一个测试框架，如下图，

<img src="../image/2024/image-20240918192140571.png" alt="image-20240918192140571" style="zoom: 67%;" />

分为两步，

**1 问题打标签**

首先给问题分类打标签，根据其答案、推理和语言的类型打标签，具体标签如下表，

<img src="../image/2024/image-20240918192309744.png" alt="image-20240918192309744" style="zoom:50%;" />

**2 回答测试**

为了解决LLM回答不能和数据集参考答案完全匹配的问题，本文提出了如下步骤，

- 将LLM的回答做一致性分解，得到一致树，提取NP、VP节点得到候选答案池
- 使用wikidata的别名列表扩大候选答案池
- 使用bert模型计算候选答案池中答案和参考答案的相似度（实验后这里设置阈值为0.78）

#### 3）实验

本文的主题实验结果如下，

![image-20240918192917770](../image/2024/image-20240918192917770.png)

参考CheckList方法，本文还进行了三个方面的实验，

**1 Minimum Functionality Test (MFT)**

只测试有一个推理类型或多个同小类推理类型的问题，结果如下，

![image-20240918193033512](../image/2024/image-20240918193033512.png)

**2 Invariance Test (INV)**

在问题中引入拼写错误

**3 Directional Expectation Test (DET)**

分为三中，

- 替换问题的推理类型
- 在问题中添加答案类型提示
- 使用CoT框架

后两者的具体示例如下图，

<img src="../image/2024/image-20240918193320717.png" alt="image-20240918193320717" style="zoom:50%;" />
